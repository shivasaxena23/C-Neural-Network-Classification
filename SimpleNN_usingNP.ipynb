{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimpleNN-usingNP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivasaxena23/C-Neural-Network-Classification/blob/master/SimpleNN_usingNP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "hDyKNGsaw2rY",
        "colab_type": "code",
        "outputId": "b1551f10-acab-4cd9-8413-8b444399ba9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install panda"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting panda\n",
            "  Downloading https://files.pythonhosted.org/packages/79/03/74996420528fe488ce17c42b6400531c8067d7eb661c304fa3aa8fdad17c/panda-0.3.1.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from panda) (40.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from panda) (2.18.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->panda) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->panda) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->panda) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->panda) (1.22)\n",
            "Building wheels for collected packages: panda\n",
            "  Building wheel for panda (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/c6/c8/45/06ed898b0bb401c1ff207dbb05b1587ff28860a236d98b1996\n",
            "Successfully built panda\n",
            "Installing collected packages: panda\n",
            "Successfully installed panda-0.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yEe-rdW2w8Px",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "import sklearn.linear_model\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "paHEvslNzl09",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of x\n",
        "    Arguments:\n",
        "    x -- A scalar or numpy array of any size.\n",
        "    Return:\n",
        "    s -- sigmoid(x)\n",
        "    \"\"\"\n",
        "    s = 1/(1+np.exp(-x))\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V8cRaeqVxC8P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_extra_datasets():  \n",
        "    N = 200\n",
        "    gaussian_quantiles = sklearn.datasets.make_gaussian_quantiles(mean=None, cov=0.7, n_samples=N, n_features=2, n_classes=2, shuffle=True, random_state=None)\n",
        "    return  gaussian_quantiles"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4bqdrYTtxG6F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gaussian_quantiles= load_extra_datasets()\n",
        "X, Y = gaussian_quantiles\n",
        "X, Y = X.T, Y.reshape(1, Y.shape[0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-B4-aObt34p4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-3jnOwrG4JqR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = df.iloc[:,:64].as_matrix()\n",
        "Y = df.iloc[:,64].as_matrix()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HWOtzFar4yqj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = X.reshape([542,64])\n",
        "Y = Y.reshape([542,1])\n",
        "X = X.T\n",
        "Y = Y.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "maMaDLzgxILG",
        "colab_type": "code",
        "outputId": "a78693cd-3958-4b54-b43e-0be91e92684f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "shape_X = X.shape\n",
        "shape_Y = Y.shape\n",
        "\n",
        "\n",
        "print ('The shape of X is: ' + str(shape_X))\n",
        "print ('The shape of Y is: ' + str(shape_Y))\n",
        "\n",
        "X.shape[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of X is: (64, 542)\n",
            "The shape of Y is: (1, 542)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "metadata": {
        "id": "3g2kMV8Tw7gt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def layer_sizes(X, Y):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- input dataset of shape (input size, number of examples)\n",
        "    Y -- labels of shape (output size, number of examples)\n",
        "    \n",
        "    Returns:\n",
        "    n_x -- the size of the input layer\n",
        "    n_h -- the size of the hidden layer\n",
        "    n_y -- the size of the output layer\n",
        "    \"\"\"\n",
        "    n_x = X.shape[0] # size of input layer`\n",
        "    n_h = 4\n",
        "    n_y =Y.shape[0] # size of output layer\n",
        "    return (n_x, n_h, n_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OyFp6KQMyDhc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# initialize_parameters\n",
        "\n",
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    n_x -- size of the input layer\n",
        "    n_h -- size of the hidden layer\n",
        "    n_y -- size of the output layer\n",
        "    \n",
        "    Returns:\n",
        "    params -- python dictionary containing your parameters:\n",
        "                    W1 -- weight matrix of shape (n_h, n_x)\n",
        "                    b1 -- bias vector of shape (n_h, 1)\n",
        "                    W2 -- weight matrix of shape (n_y, n_h)\n",
        "                    b2 -- bias vector of shape (n_y, 1)\n",
        "    \"\"\"\n",
        "        \n",
        "    W1 = np.random.randn(n_h,n_x) * 0.01\n",
        "    b1 = np.zeros(shape=(n_h, 1))\n",
        "    W2 = np.random.randn(n_y,n_h) * 0.01\n",
        "    b2 = np.zeros(shape=(n_y, 1))\n",
        "    \n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZgN7wF2xyQXy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#forward_propagation\n",
        "\n",
        "def forward_propagation(X, parameters):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    X -- input data of size (n_x, m)\n",
        "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
        "    \n",
        "    Returns:\n",
        "    A2 -- The sigmoid output of the second activation\n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
        "    \"\"\"\n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    \n",
        "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
        "    Z1 = np.dot(W1,X) + b1\n",
        "    A1 = np.tanh(Z1)\n",
        "    Z2A = np.dot(W2,A1)\n",
        "    Z2 = np.dot(W2,A1) + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "    \n",
        "    cache = {\"Z1\": Z1,\n",
        "             \"A1\": A1,\n",
        "             \"Z2\": Z2,\n",
        "             \"A2\": A2}\n",
        "    \n",
        "    return A2, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gUTHgV6DyRGL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Gcompute_cost\n",
        "\n",
        "def compute_cost(A2, Y, parameters):\n",
        "    \"\"\"\n",
        "    \n",
        "    Arguments:\n",
        "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
        "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
        "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
        "    \n",
        "    Returns:\n",
        "    cost -- cross-entropy cost given equation (13)\n",
        "    \"\"\"\n",
        "    \n",
        "    m = Y.shape[1] # number of example\n",
        "\n",
        "    # Compute the cross-entropy cost\n",
        "    logprobs = np.multiply(np.log(A2), Y) + np.multiply((1 - Y), np.log(1 - A2))\n",
        "    cost = - np.sum(logprobs) / m    \n",
        "    \n",
        "    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n",
        "                                # E.g., turns [[17]] into 17 \n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FMI8wvIzyTzu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# backward_propagation\n",
        "\n",
        "def backward_propagation(parameters, cache, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation using the instructions above.\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing our parameters \n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
        "    X -- input data of shape (2, number of examples)\n",
        "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
        "    \n",
        "    Returns:\n",
        "    grads -- python dictionary containing your gradients with respect to different parameters\n",
        "    \"\"\"\n",
        "    m = X.shape[1]\n",
        "    \n",
        "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
        "    W1 = parameters['W1']\n",
        "    W2 = parameters['W2']\n",
        "        \n",
        "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
        "    A1 = cache['A1']\n",
        "    A2 = cache['A2']\n",
        "    \n",
        "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
        "\n",
        "    dZ2 = A2 - Y\n",
        "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
        "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n",
        "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
        "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "    \n",
        "    grads = {\"dW1\": dW1,\n",
        "             \"db1\": db1,\n",
        "             \"dW2\": dW2,\n",
        "             \"db2\": db2}\n",
        "    \n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CbV7_SNlyZF-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# update_parameters\n",
        "\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
        "    \"\"\"\n",
        "    Updates parameters using the gradient descent update rule given above\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients \n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "    \"\"\"\n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "\n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "\n",
        "    \n",
        "    # Retrieve each gradient from the dictionary \"grads\"\n",
        "    dW1 = grads['dW1']\n",
        "    db1 = grads['db1']\n",
        "    dW2 = grads['dW2']\n",
        "    db2 = grads['db2']\n",
        "\n",
        "    \n",
        "    # Update rule for each parameter\n",
        "\n",
        "    W1 = W1 - learning_rate * dW1\n",
        "    b1 = b1 - learning_rate * db1\n",
        "    W2 = W2 - learning_rate * dW2\n",
        "    b2 = b2 - learning_rate * db2\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hPtIxQW_yb7D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# nn_model\n",
        "\n",
        "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- dataset of shape (2, number of examples)\n",
        "    Y -- labels of shape (1, number of examples)\n",
        "    n_h -- size of the hidden layer\n",
        "    num_iterations -- Number of iterations in gradient descent loop\n",
        "    print_cost -- if True, print the cost every 1000 iterations\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(3)\n",
        "    n_x = layer_sizes(X, Y)[0]\n",
        "    n_y = layer_sizes(X, Y)[2]\n",
        "    \n",
        "    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n",
        "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "         \n",
        "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
        "        A2, cache = forward_propagation(X, parameters)\n",
        "        \n",
        "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
        "        cost = compute_cost(A2, Y, parameters)\n",
        " \n",
        "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
        "        grads = backward_propagation(parameters, cache, X, Y)\n",
        " \n",
        "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
        "        parameters = update_parameters(parameters, grads)\n",
        "        \n",
        "        # Print the cost every 1000 iterations\n",
        "        if print_cost and i % 1000 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n2QAcn4Fyckb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# predict\n",
        "\n",
        "def predict(parameters, X):\n",
        "    \"\"\"\n",
        "    Using the learned parameters, predicts a class for each example in X\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    X -- input data of size (n_x, m)\n",
        "    \n",
        "    Returns\n",
        "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
        "    ### START CODE HERE ### (≈ 2 lines of code)\n",
        "    A2, cache = forward_propagation(X,parameters)\n",
        "    predictions = A2 > 0.5\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return predictions\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T8ydMxJUy38t",
        "colab_type": "code",
        "outputId": "d51a0aeb-e24e-4a60-bcc1-eb6b58c92a09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "parameters = nn_model(X, Y, n_h = 11, num_iterations = 5000, print_cost=True)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 0.693482\n",
            "Cost after iteration 1000: 0.596598\n",
            "Cost after iteration 2000: 0.596598\n",
            "Cost after iteration 3000: 0.596598\n",
            "Cost after iteration 4000: 0.596598\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ek4_qHP9yfX2",
        "colab_type": "code",
        "outputId": "ac42566b-60ec-4f1b-f7a9-9398d0731d91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Print accuracy\n",
        "predictions = predict(parameters, X)\n",
        "predictions.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 542)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "metadata": {
        "id": "5wCxmHIj6L8N",
        "colab_type": "code",
        "outputId": "4835fe2a-da44-41b9-b2e9-52c969a24f2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "print ('Accuracy: %d' % float((np.sum(predictions == Y)/Y.shape[1])*100) + '%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 71%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Kzw6GFXi7Okt",
        "colab_type": "code",
        "outputId": "c6e7502e-0fe1-48ce-9a8b-422515f9aa56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "predictions.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 542)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "tCse-WL1aMmz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mew8z4YYynzb",
        "colab_type": "code",
        "outputId": "99a67871-1b4e-4527-f814-f0df37beada7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        }
      },
      "cell_type": "code",
      "source": [
        "# Running the model with diffrent number of neurons in the hiddern layer\n",
        "\n",
        "plt.figure(figsize=(16, 32))\n",
        "hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]\n",
        "for i, n_h in enumerate(hidden_layer_sizes):\n",
        "    plt.subplot(5, 2, i+1)\n",
        "    plt.title('Hidden Layer of size %d' % n_h)\n",
        "    parameters = nn_model(X, Y, n_h, num_iterations = 5000)\n",
        "    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n",
        "    predictions = predict(parameters, X)\n",
        "    accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)\n",
        "    print (\"Accuracy for {} hidden units: {} %\".format(n_h, accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-8f3564c392d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Hidden Layer of size %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mn_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mplot_decision_boundary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plot_decision_boundary' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcAAAAFaCAYAAACJ2I1WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFExJREFUeJzt3XuQnXV5wPHvQgRFNhbpQS7qgDY+\nEryMicVkgIBgKSNqi41VpzoNBosSK16pgzjVOgNWzaRSWy11HOwFpNIBQ4maagvFhiqXioj4IGLw\nsgwsl4aMo2LC9o/zBk+Wze7JnnP2kuf7mclwznl/55wfP3b57vued98MjY2NIUlSNXvN9gQkSZoN\nBlCSVJIBlCSVZAAlSSUZQElSSQZQklSSAdS8EhFjEfH0cY+tioivNrffFhEf3sVz74yIEyZ4/NiI\n2NzHOV4cEef16/X6LSJeEhE/joirduM534uIp/Xp/fePiH+OiG39eD1puhbM9gSkfsrMT872HOaB\nk4FrMvON3T4hM5/bx/ffBPxbH19PmhYDqD1KRHwQeHpmnhERS4F/AJ4AXD1u3HnAmcD9wPqOx/cF\nPgacAuwDXJSZ5zfbNgMXAKuBZwCXZOa7d3N+ZwDvpv29dw/wRuBh4KfAEZl5bzPu482YdwIfAP4I\neCJwJfCuzNweEdcA/w28GlidmZvGvdfbgbfQPtKTwBnA8cDZwIKI2JCZLx/3nLcBa4ChZl6nZ+Zt\nETHW/Du/HXhVM3wvYBHwgsy8NSL+BHhXM8/rgTdl5s8nWIYzm3/3c3Zn7aR+8xCo9mSfAj6Rmc+h\nvddxBEBELKb9P+oXN39e0PGcc4DFwPOBo4CVEfGKju0rgOXAUuBPxx+OnUxEHAR8EvidzFwE3Al8\nIDMfAr4KvLZj+GnA54E3AH8IHA08u/nz1o5xS4GjJojfMuC9wAnN3tuPgAsy8/JmDpdPEL9h4MPA\n0c1zPgac2jkmM8/JzOc22z8HrG/id1zz3BMz83BgS3P/cTLz+ikXS5oBBlDz0TXNZ1Lfi4jv0d4r\n20lEPBH4beCy5qHLgZ81t1cA12bmvZm5Hfinjqe+EvjbzPxlZv6M9h7kqzu2X5KZ2zNzBLiX9l5R\nVzLzPmBhZv6keeg64FnN7UuB1zdzfwGwd2b+TzOfz2bmlszcBnxm3Hw2ZOajE7zdqbQjd19z/zO0\nD31O5hfAGLA6Ip6WmV/IzI9ONDAilgNvBt7UPPRK4LJmXQA+PW6e0pzjIVDNRyd0RISIWEV7T6nT\nU5t/PgyQmWMR8X8d27Z0jH2o4/ZvAOsi4vzm/r7ANzu2dz5vO7B3t5OOiL2Bv4iIVzXPGwbuaDav\nB/4+Io4Afh/4l475vKc5vAjt79nRjpd9cBdv1wJGOu4/BBw02fwy81cRcRJwLvChiPg2cFZm3jru\n3+MpwD8CqzLzgY55nhYROyK7F+1DyNKcZQC1p9oRtYXAlojYi19H8SHgKR1jWx23R4CPZ+YgTtJ4\nLe3Pz1Zk5v0R8Wban+2RmT9rzsp8DbASOL1jPuuncXLPvcCBHfcPbB6bVGb+L/CaiNiH9uHgTwPH\njBv2d8DnM/OajsdGgM9l5nt2c57SrPEQqPZIzckXt9D+LA3gdbRPzoD2CRrHRkSr2Svr3Hv8InBG\nROwdEUMRcV5EnNKnaR0EbG7idyDtz/b279h+CXAWsF9m3tQxnzdGxH4AEXFmRPxxF+91NfDq5n2g\nfeLJ1ZOMJyKeHxFfiIh9MvMR4Ebah0Q7x6wGngl8cNzT1zfv12rG/V5E/FkX85RmjXuA2pO9Ffhs\nRJwLbAC+C5CZ34qITwM3Aw/Q/vzt+c1z/gY4HLiN9pmQNwJ/NY33PjsiOsN6NfBR4PURcSdwF3Ae\nsD4i1jZnk36F9h7rpzqedyXtk3FujgiAH9A+C3VSmfnNiPgIcF2z9/stdj55ZiLfAX4I3BYRjwBb\naZ8R2ulc4EnAd5r5APx5Zl7WHDa+pnm/+2hHdycRsYR26J8A7N18htvvX7OQujLk3wcozR0RcRvw\nmsz87mzPRdrTeQhUmiMi4nXAPcZPmhldHQKNiOfR/ixi3fgP4yPiZcD5tM+I25CZE/7uj6Rdi4h/\nB36T9gkwkmbAlIdAI+LJtC9b9H3g2xME8LvA79K+ksW1wJn+BCtJmuu6OQT6S+Dl7Pw7RQBExLOA\nBzPzx80v424ATurvFCVJ6r8pA5iZ23ZxPT+Ag9n5l3LvAw7px8QkSRqkfv8axNBUA8bGxsaGhqYc\nJklSt6YVlV4DOEJ7L3CHw5jgUGmnoaEhRke39vi2tbVaw65hH7iOvXMNe+ca9q7VGp7W83r6NYjM\n3AwsjIjDI2IB8ApgYy+vKUnSTJhyD7D5O9XW0r46xq8iYiXtyx79MDOvoH11iUub4Zdl5h0TvpAk\nSXPIlAFsrkl4wiTb/4v2348mSdK84ZVgJEklGUBJUkkGUJJUkgGUJJVkACVJJRlASVJJBlCSVJIB\nlCSVZAAlSSUZQElSSQZQklSSAZQklWQAJUklGUBJUkkGUJJUkgGUJJVkACVJJRlASVJJBlCSVJIB\nlCSVZAAlSSUZQElSSQZQklSSAZQklWQAJUklGUBJUkkGUJJUkgGUJJVkACVJJRlASVJJBlCSVJIB\nlCSVZAAlSSUZQElSSQZQklSSAZQklWQAJUklGUBJUkkGUJJUkgGUJJVkACVJJRlASVJJBlCSVJIB\nlCSVZAAlSSUZQElSSQZQklSSAZQklWQAJUklGUBJUkkGUJJU0oJuBkXEOmAZMAacnZk3dGxbA7wB\n2A7cmJnvGMREJUnqpyn3ACPieGBRZi4HVgMXdmxbCLwXOC4zjwUWR8SyQU1WkqR+6eYQ6EnAlQCZ\neTtwQBM+gEeaP/tHxAJgP+DBQUxUkqR+6iaABwOjHfdHm8fIzF8AHwLuAu4GvpGZd/R7kpIk9VtX\nnwGOM7TjRrMneC7wHOBh4D8i4oWZectkL9BqDU/jbdXJNewP17F3rmHvXMPZ0U0AR2j2+BqHAvc0\nt48E7srM+wEi4jpgKTBpAEdHt+7+TPWYVmvYNewD17F3rmHvXMPeTfcHiG4OgW4EVgJExBJgJDN3\n/NfaDBwZEU9q7r8Y+P60ZiJJ0gyacg8wMzdFxE0RsQl4FFgTEauALZl5RUR8DPjPiNgGbMrM6wY7\nZUmSejc0NjY20+855u5+bzxk0h+uY+9cw965hr1rtYaHph71eF4JRpJUkgGUJJVkACVJJRlASVJJ\nBlCSVJIBlCSVZAAlSSUZQElSSQZQklSSAZQklWQAJUklGUBJUkkGUJJUkgGUJJVkACVJJRlASVJJ\nBlCSVJIBlCSVZAAlSSUZQElSSQZQklSSAZQklWQAJUklGUBJUkkGUJJUkgGUJJVkACVJJRlASVJJ\nBlCSVJIBlCSVZAAlSSUZQElSSQZQklSSAZQklWQAJUklGUBJUkkGUJJUkgGUJJVkACVJJRlASVJJ\nBlCSVJIBlCSVZAAlSSUZQElSSQZQklSSAZQklWQAJUklGUBJUkkGUJJUkgGUJJVkACVJJS3oZlBE\nrAOWAWPA2Zl5Q8e2ZwCXAvsAN2fmWwYxUUmS+mnKPcCIOB5YlJnLgdXAheOGrAXWZubRwPaIeGb/\npylJUn91cwj0JOBKgMy8HTggIhYCRMRewHHA+mb7msz80YDmKklS33QTwIOB0Y77o81jAC1gK7Au\nIr4eERf0eX6SJA1EV58BjjM07vZhwCeAzcDVEXFqZl492Qu0WsPTeFt1cg37w3XsnWvYO9dwdnQT\nwBF+vccHcChwT3P7fuDuzPwBQER8DTgKmDSAo6Nbd3+mekyrNewa9oHr2DvXsHeuYe+m+wNEN4dA\nNwIrASJiCTCSmVsBMnMbcFdELGrGLgVyWjORJGkGTbkHmJmbIuKmiNgEPAqsiYhVwJbMvAJ4B3Bx\nc0LMrcBVg5ywJEn90NVngJn5vnEP3dKx7U7g2H5OSpKkQfNKMJKkkgygJKkkAyhJKskASpJKMoCS\npJIMoCSpJAMoSSrJAEqSSjKAkqSSDKAkqSQDKEkqyQBKkkoygJKkkgygJKkkAyhJKskASpJKMoCS\npJIMoCSpJAMoSSrJAEqSSjKAkqSSDKAkqSQDKEkqyQBKkkoygJKkkgygJKkkAyhJKskASpJKMoCS\npJIMoCSpJAMoSSrJAEqSSjKAkqSSDKAkqSQDKEkqyQBKkkoygJKkkgygJKkkAyhJKskASpJKMoCS\npJIMoCSpJAMoSSrJAEqSSjKAkqSSDKAkqSQDKEkqyQBKkkoygJKkkgygJKkkAyhJKmlBN4MiYh2w\nDBgDzs7MGyYYcwGwPDNP6OsMJUkagCn3ACPieGBRZi4HVgMXTjBmMbCi/9OTJGkwujkEehJwJUBm\n3g4cEBELx41ZC7y/z3OTJGlgugngwcBox/3R5jEAImIVcC2wuZ8TkyRpkLr6DHCcoR03IuKpwOnA\ny4DDun2BVmt4Gm+rTq5hf7iOvXMNe+cazo5uAjhCxx4fcChwT3P7RKAFXAfsCzw7ItZl5jsne8HR\n0a3TmKp2aLWGXcM+cB175xr2zjXs3XR/gOjmEOhGYCVARCwBRjJzK0BmXp6ZizNzGXAacPNU8ZMk\naS6YMoCZuQm4KSI20T4DdE1ErIqI0wY+O0mSBqSrzwAz833jHrplgjGbgRN6n5IkSYPnlWAkSSUZ\nQElSSQZQklSSAZQklWQAJUklGUBJUkkGUJJUkgGUJJVkACVJJRlASVJJBlCSVJIBlCSVZAAlSSUZ\nQElSSQZQklSSAZQklWQAJUklGUBJUkkGUJJUkgGUJJVkACVJJRlASVJJBlCSVJIBlCSVZAAlSSUZ\nQElSSQZQklSSAZQklWQAJUklGUBJUkkGUJJUkgGUJJVkACVJJRlASVJJBlCSVJIBlCSVZAAlSSUZ\nQElSSQZQklSSAZQklWQAJUklGUBJUkkGUJJUkgGUJJVkACVJJRlASVJJBlCSVJIBlCSVZAAlSSUZ\nQElSSQZQklTSgm4GRcQ6YBkwBpydmTd0bHspcAGwHUjgjMx8dABzlSSpb6bcA4yI44FFmbkcWA1c\nOG7IRcDKzDwGGAZO6fssJUnqs24OgZ4EXAmQmbcDB0TEwo7tSzPzJ83tUeDA/k5RkqT+6yaAB9MO\n2w6jzWMAZObDABFxCHAysKGfE5QkaRC6+gxwnKHxD0TEQcBVwFmZ+cBUL9BqDU/jbdXJNewP17F3\nrmHvXMPZ0U0AR+jY4wMOBe7Zcac5HPol4P2ZubGbNx0d3bo7c9Q4rdawa9gHrmPvXMPeuYa9m+4P\nEN0cAt0IrASIiCXASGZ2/tdaC6zLzC9PawaSJM2CobGxsSkHRcRHgBXAo8Aa4EXAFuArwEPA9R3D\nL8nMiyZ5uTF/2umNPzH2h+vYO9ewd65h71qt4cd9NNeNrj4DzMz3jXvolo7b+07njSVJmk1eCUaS\nVJIBlCSVZAAlSSUZQElSSQZQklSSAZQklWQAJUklGUBJUkkGUJJUkgGUJJVkACVJJRlASVJJBlCS\nVJIBlCSVZAAlSSUZQElSSQZQklSSAZQklWQAJUklGUBJUkkGUJJUkgGUJJVkACVJJRlASVJJBlCS\nVJIBlCSVZAAlSSUZQElSSQZQklSSAZQklWQAJUklGUBJUkkGUJJUkgGUJJVkACVJJRlASVJJBlCS\nVJIBlCSVZAAlSSUZQElSSQZQklSSAZQklWQAJUklGUBJUkkGUJJUkgGUJJVkACVJJRlASVJJBlCS\nVJIBlCSVZAAlSSUt6GZQRKwDlgFjwNmZeUPHtpcB5wPbgQ2Z+eFBTFSSpH6acg8wIo4HFmXmcmA1\ncOG4IRcCfwAcA5wcEYv7PktJkvqsm0OgJwFXAmTm7cABEbEQICKeBTyYmT/OzEeBDc14SZLmtG4C\neDAw2nF/tHlsom33AYf0Z2qSJA1OV58BjjM0zW2PjWm1hqfxturkGvaH69g717B3ruHs6GYPcIRf\n7/EBHArcs4tthzWPSZI0p3UTwI3ASoCIWAKMZOZWgMzcDCyMiMMjYgHwima8JElz2tDY2NiUgyLi\nI8AK4FFgDfAiYEtmXhERK4C/bIb+a2Z+fFCTlSSpX7oKoCRJexqvBCNJKskASpJKms6vQXTNS6j1\nboo1fClwAe01TOCM5oIE6jDZGnaMuQBYnpknzPD05oUpvg6fAVwK7APcnJlvmZ1Zzm1TrOEa4A20\nv5dvzMx3zM4s576IeB7wRWBdZn5y3Lbd6srA9gC9hFrvuljDi4CVmXkMMAycMsNTnPO6WEOar70V\nMz23+aKLNVwLrM3Mo4HtEfHMmZ7jXDfZGjZX1novcFxmHgssjohlszPTuS0ingz8NfC1XQzZra4M\n8hCol1Dr3S7XsLE0M3/S3B4FDpzh+c0HU60htP8H/v6Zntg8Mtn38l7AccD6ZvuazPzRbE10Dpvs\n6/CR5s/+za+T7Qc8OCuznPt+CbycCX7ffDpdGWQAvYRa7yZbQzLzYYCIOAQ4mfZ/cO1s0jWMiFXA\ntcDmGZ3V/DLZGraArcC6iPh6cyhZj7fLNczMXwAfAu4C7ga+kZl3zPgM54HM3JaZP9/F5t3uykye\nBNPrJdQ0wTpFxEHAVcBZmfnAzE9p3nlsDSPiqcDptPcA1b2hcbcPAz4BHA+8KCJOnZVZzS+dX4cL\ngXOB5wBHAC+JiBfO1sT2IFN2ZZAB9BJqvZtsDXd843wJOC8zvQLPxCZbwxNp78FcB1wBLGlOVNDO\nJlvD+4G7M/MHmbmd9mczR83w/OaDydbwSOCuzLw/Mx+h/fW4dIbntyfY7a4MMoBeQq13u1zDxlra\nZ0J9eTYmN09M9nV4eWYuzsxlwGm0z2B85+xNdc6abA23AXdFxKJm7FLaZyRrZ5N9L28GjoyIJzX3\nXwx8f8ZnOM9NpysDvRKMl1Dr3a7WEPgK8BBwfcfwSzLzohmf5Bw32ddhx5jDgYv9NYiJTfG9/FvA\nxbR/oL4VeKu/jvN4U6zhmbQPx28DNmXmObM307krIpbS/sH/cOBXwE9pn4D1w+l0xUuhSZJK8kow\nkqSSDKAkqSQDKEkqyQBKkkoygJKkkgygJKkkAyhJKskASpJK+n8mQqeFNE7OFAAAAABJRU5ErkJg\ngg==\n",
            "text/plain": [
              "<Figure size 1152x2304 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "orS8acdlz8rs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}